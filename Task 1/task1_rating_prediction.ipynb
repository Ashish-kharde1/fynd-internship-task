{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ff0a1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adb972b",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    api_key=\"OPENROUTER_API_KEY\",\n",
    "    base_url=\"https://openrouter.ai/api/v1\"\n",
    ")\n",
    "\n",
    "MODEL_NAME = \"mistralai/mistral-7b-instruct\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cac20852",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_text</th>\n",
       "      <th>true_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We got here around midnight last Friday... the...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Brought a friend from Louisiana here.  She say...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Every friday, my dad and I eat here. We order ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>My husband and I were really, really disappoin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Love this place!  Was in phoenix 3 weeks for w...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         review_text  true_rating\n",
       "0  We got here around midnight last Friday... the...            4\n",
       "1  Brought a friend from Louisiana here.  She say...            5\n",
       "2  Every friday, my dad and I eat here. We order ...            3\n",
       "3  My husband and I were really, really disappoin...            1\n",
       "4  Love this place!  Was in phoenix 3 weeks for w...            5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"yelp.csv\")\n",
    "\n",
    "df = df[[\"text\", \"stars\"]].rename(\n",
    "    columns={\"text\": \"review_text\", \"stars\": \"true_rating\"}\n",
    ")\n",
    "\n",
    "df = df.sample(200, random_state=42).reset_index(drop=True)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b66e56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_v1 = \"\"\"\n",
    "Given the following restaurant review, predict a rating from 1 to 5 stars.\n",
    "Return JSON with predicted_stars and explanation.\n",
    "\n",
    "Review:\n",
    "{review_text}\n",
    "\"\"\"\n",
    "prompt_v2 = \"\"\"\n",
    "Classify the sentiment of the following restaurant review and predict a star rating from 1 to 5.\n",
    "\n",
    "Respond with a single JSON object only.\n",
    "Do not include any extra text.\n",
    "\n",
    "Fields required:\n",
    "- predicted_stars (integer 1–5)\n",
    "- explanation (short reason)\n",
    "\n",
    "Review:\n",
    "{review_text}\n",
    "\"\"\"\n",
    "\n",
    "prompt_v3 = \"\"\"\n",
    "Rate the restaurant review using this rubric:\n",
    "\n",
    "1 = very negative\n",
    "2 = mostly negative\n",
    "3 = mixed\n",
    "4 = mostly positive\n",
    "5 = very positive\n",
    "\n",
    "Respond with a single JSON object only.\n",
    "Do not include any extra text.\n",
    "\n",
    "Fields required:\n",
    "- predicted_stars (integer 1–5)\n",
    "- explanation (short reason)\n",
    "\n",
    "Review:\n",
    "{review_text}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0d096c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def run_llm(prompt_template, review_text):\n",
    "    try:\n",
    "        prompt = prompt_template.format(review_text=review_text)\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=MODEL_NAME,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.3\n",
    "        )\n",
    "\n",
    "        raw_text = response.choices[0].message.content\n",
    "\n",
    "        # ---- ROBUST JSON EXTRACTION ----\n",
    "        match = re.search(r\"\\{[\\s\\S]*\\}\", raw_text)\n",
    "        if not match:\n",
    "            raise ValueError(\"No JSON found\")\n",
    "\n",
    "        json_str = match.group(0)\n",
    "        parsed = json.loads(json_str)\n",
    "\n",
    "        return {\n",
    "            \"predicted_stars\": int(parsed[\"predicted_stars\"]),\n",
    "            \"explanation\": parsed.get(\"explanation\", \"\"),\n",
    "            \"json_valid\": True\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"predicted_stars\": None,\n",
    "            \"explanation\": None,\n",
    "            \"json_valid\": False\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "085170d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'predicted_stars': None, 'explanation': None, 'json_valid': False}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_llm(prompt_v3, df.iloc[0][\"review_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15ce3e42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "prompt\n",
       "v1    176\n",
       "v2    189\n",
       "v3    142\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts = {\n",
    "    \"v1\": prompt_v1,\n",
    "    \"v2\": prompt_v2,\n",
    "    \"v3\": prompt_v3\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    for name, prompt in prompts.items():\n",
    "        res = run_llm(prompt, row[\"review_text\"])\n",
    "\n",
    "        results.append({\n",
    "            \"prompt\": name,\n",
    "            \"true_rating\": row[\"true_rating\"],\n",
    "            \"predicted_rating\": res[\"predicted_stars\"],\n",
    "            \"json_valid\": res[\"json_valid\"]\n",
    "        })\n",
    "\n",
    "        time.sleep(0.1)  # safe for free tier\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.head()\n",
    "# Keep only rows where prediction exists\n",
    "valid_results = results_df.dropna(subset=[\"predicted_rating\"])\n",
    "\n",
    "valid_results.groupby(\"prompt\").size()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "784a7b3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "prompt\n",
       "v1    0.596591\n",
       "v2    0.613757\n",
       "v3    0.640845\n",
       "Name: correct, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uca = (\n",
    "    valid_results\n",
    "    .assign(correct=lambda x: x.predicted_rating == x.true_rating)\n",
    "    .groupby(\"prompt\")[\"correct\"]\n",
    "    .mean()\n",
    ")\n",
    "\n",
    "uca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b21478fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "prompt\n",
       "v1    0.880\n",
       "v2    0.945\n",
       "v3    0.710\n",
       "Name: json_valid, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_validity = results_df.groupby(\"prompt\")[\"json_valid\"].mean()\n",
    "json_validity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3636dc2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>consistency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>v1</td>\n",
       "      <td>0.823529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>v2</td>\n",
       "      <td>0.850000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>v3</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  prompt  consistency\n",
       "0     v1     0.823529\n",
       "1     v2     0.850000\n",
       "2     v3     1.000000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_consistency_safe(p1, p2):\n",
    "    pairs = [(a, b) for a, b in zip(p1, p2) if a is not None and b is not None]\n",
    "    if len(pairs) == 0:\n",
    "        return 0.0\n",
    "    return sum(a == b for a, b in pairs) / len(pairs)\n",
    "\n",
    "\n",
    "\n",
    "# Consistency evaluation on repeated runs\n",
    "sample_df = df.sample(20, random_state=1)\n",
    "\n",
    "consistency_rows = []\n",
    "\n",
    "for name, prompt in prompts.items():\n",
    "    preds_1, preds_2 = [], []\n",
    "\n",
    "    for _, row in sample_df.iterrows():\n",
    "        preds_1.append(run_llm(prompt, row[\"review_text\"])[\"predicted_stars\"])\n",
    "        preds_2.append(run_llm(prompt, row[\"review_text\"])[\"predicted_stars\"])\n",
    "        time.sleep(0.1)\n",
    "\n",
    "    consistency_rows.append({\n",
    "        \"prompt\": name,\n",
    "        \"consistency\": compute_consistency_safe(preds_1, preds_2)\n",
    "    })\n",
    "\n",
    "consistency_df = pd.DataFrame(consistency_rows)\n",
    "consistency_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99ce48f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>json_validity</th>\n",
       "      <th>consistency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>v1</td>\n",
       "      <td>0.596591</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.823529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>v2</td>\n",
       "      <td>0.613757</td>\n",
       "      <td>0.945</td>\n",
       "      <td>0.850000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>v3</td>\n",
       "      <td>0.640845</td>\n",
       "      <td>0.710</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  prompt  accuracy  json_validity  consistency\n",
       "0     v1  0.596591          0.880     0.823529\n",
       "1     v2  0.613757          0.945     0.850000\n",
       "2     v3  0.640845          0.710     1.000000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_table = pd.concat(\n",
    "    [\n",
    "        uca.rename(\"accuracy\"),\n",
    "        json_validity.rename(\"json_validity\")\n",
    "    ],\n",
    "    axis=1\n",
    ").reset_index()\n",
    "\n",
    "final_table = final_table.merge(consistency_df, on=\"prompt\")\n",
    "final_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca55db4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
